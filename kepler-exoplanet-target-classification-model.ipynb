{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1995,"sourceType":"datasetVersion","datasetId":1074},{"sourceId":4877,"sourceType":"datasetVersion","datasetId":2894},{"sourceId":6582170,"sourceType":"datasetVersion","datasetId":3800144}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/kepler-exoplanet-classification-model?scriptVersionId=216586085\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Kepler Data Analysis","metadata":{}},{"cell_type":"code","source":"# lets begin!\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n# for the sake of expeditious analysis\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom wordcloud import WordCloud, STOPWORDS\nfrom geopandas import GeoDataFrame\nimport matplotlib.colors as colors\nimport seaborn as sns\nimport random as r\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# kepler labled test & train\nexoTest = pd.read_csv('/kaggle/input/kepler-labelled-time-series-data/exoTest.csv',header=0)\nexoTrain = pd.read_csv('/kaggle/input/kepler-labelled-time-series-data/exoTrain.csv',header=0)\ncumulative = pd.read_csv('/kaggle/input/kepler-exoplanet-search-results/cumulative.csv',header=0)\ncomposite = pd.read_csv(\"/kaggle/input/httpsexoplanetarchive-ipac-caltech-edu/PSCompPars_2023.09.30_20.49.04.csv\",header=168)#,error_bad_lines=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-01-08T03:07:45.874598Z","iopub.execute_input":"2025-01-08T03:07:45.875197Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ncomposite.discoverymethod.value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncomposite.discoverymethod.value_counts().plot(kind='bar',figsize=(15,3),color='pink')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Multidimensional ExoPlots","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(composite, x='elat', y='elon', z='glat',\n              color='disc_year',\n              size = 'ra',\n              hover_name = 'hostname',\n              hover_data=['disc_locale','disc_telescope','gaia_id','hd_name','disc_facility'],              \n              opacity=0.5,\n              size_max=25\n                   )\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(composite, x='elat', y='elon', z='glon',\n              color='disc_year',\n              size = 'ra',\n              hover_name = 'hostname',\n              hover_data=['disc_locale','disc_telescope','gaia_id','hd_name','disc_facility'],              \n              opacity=0.5,\n              size_max=25\n                   )\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# datasets\n#exoTest.describe()#.style.background_gradient(cmap ='coolwarm').set_properties(**{'font-size': '8px'})\n#exoTrain.describe()#.style.background_gradient(cmap ='coolwarm').set_properties(**{'font-size': '8px'})\n#cumulative.describe()#.style.background_gradient(cmap ='coolwarm').set_properties(**{'font-size': '8px'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exoplanet Dataset","metadata":{}},{"cell_type":"code","source":"exoTest","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"exoTest['FLUX.1'].value_counts().head(20).plot(kind='bar',figsize=(15,3),color='lightblue')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Groupby by LABEL\nshape = exoTest.groupby(\"LABEL\")\n\n# Summary statistic of all countries\nshape.describe().style.background_gradient(cmap ='coolwarm').set_properties(**{'font-size': '8px'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ML\nWe have to encode the composite exoplanet dataset prior to running a machine learning model against it.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef encode(df):\n    lb_make = LabelEncoder()\n    columns = df.columns.values.tolist()\n    df_encoded = df[columns].copy()\n\n    # categorize/encode\n    for i in columns:\n        df_encoded[i] = lb_make.fit_transform(df[i])\n\n    # encoded\n    return df_encoded\n\n# set x & y variables\nx = composite.drop(columns='discoverymethod')\ny = composite['discoverymethod'].ravel()\n\nX_train, X_test, y_train, y_test = train_test_split(x, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose target variable\n#target = input(\"Enter target variable: \")\ntarget = 'discoverymethod'#\"koi_disposition\"\n\n# quick proof of concept\na = encode(composite.copy())\n\n# find random sample\nfrom random import randrange\nidx = randrange(len(a))\n\n# print random configuration item\nb = pd.DataFrame(a.loc[idx]).T\nprint(f\"{target}:\",b.reset_index()[target][0])\n\n# store sol'n\nsolution = b.reset_index()[target][0]\n\n# print data point\nb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# categorize/encode entire dataframe(a)\nc = encode(a)\nprint(\"\\nOriginal dataframe encoded.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print encoded item\nuse_case = pd.DataFrame(c.loc[idx]).T.drop(columns=[target]) \n#c\n\n# print encoded item w/out target info\ndata = c.drop(columns=[target]) \nuse_case","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def X_y_sets(df, target):\n    #X = df.dropna().drop(columns=[target]).copy()\n    #y = df.dropna()[target].ravel().copy()\n    \n    # w.o dropna()\n    X = df.drop(columns=[target]).copy()\n    y = df.dropna()[target].ravel().copy()\n    \n    return train_test_split(X, y, test_size=0.33, random_state=42), X, y# save trainer\nprint(\"\\nResetting train data...\")\ntrainer = c.loc[c.index!=idx].dropna().copy()\nX, y =  trainer.drop(columns=[target]), trainer[target].ravel()\nX_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encoded variable re-mapping\n# specific to our current target choice\nd = encoding_remap(a, c, target)\nprint(\"\\nDecoding our encoded dataframe to correlate with the initial randomly chosen subject.\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nLive prediction\\n\")\n\n# choose classifier\nclf = GradientBoostingClassifier(criterion=\"friedman_mse\", init=None, learning_rate=0.033, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=60, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=100, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n#clf = RandomForestClassifier(max_depth=5, n_estimators=200, random_state=42).fit(X_train, y_train)\n#clf = ExtraTreesClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n#clf = AdaBoostClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n#clf = MLPClassifier(alpha=0.666, max_iter=666).fit(X_train, y_train)\n#clf = KNeighborsClassifier().fit(X_train, y_train)\n\n\nprint()\nprint(\"Test score: \",clf.score(X_test, y_test))\nprint()\nprediction = clf.predict(use_case)[0]\nprint(f\"Prediction {target}:\",prediction)\n\n\n# print decoded prediction\nprint(\"\\nPrediction Decoded\")\ne = d[d.index == prediction]\ne","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if solution == e.reset_index()[target][0]:\n    print(\"Machine's prediction was correct!\")\nelse:\n    print(\"Machine's prediction was incorrect :(\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pairplot Against encoded(composite) & Target Variable\nThe cell block below takes several minutes to compute.","metadata":{}},{"cell_type":"code","source":"pairplot(encode(x),y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Supplementary","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# encoding\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode(df):\n    lb_make = LabelEncoder()\n    columns = df.columns.values.tolist()\n    df_encoded = df[columns].copy()\n\n    # categorize/encode\n    for i in columns:\n        df_encoded[i] = lb_make.fit_transform(df[i])\n\n    # encoded\n    return df_encoded\n\n\n# encoded variable re-mapping\ndef encoding_remap(df, df_encoded, target):\n    \n    X_test = X_y_sets(df, target)[0][0]\n    \n    remap = pd.merge(df_encoded.loc[df_encoded.index.isin(X_test.index.values)][target].reset_index(),\n              df.loc[df.index.isin(X_test.index.values)][[target]].reset_index(),on=['index'])\n    \n    remap[target] = [str(remap[f'{target}_y'][i]) for i,v in remap[f'{target}_x'].items()]\n    remap['index'] = np.array([str(remap[f'{target}_x'][i]) for i,v in remap[f'{target}_x'].items()]).astype(int)\n    remap=remap[[target,'index']]\n    remap = remap.set_index('index').drop_duplicates().sort_values('index')\n    \n    return remap\n\n\n# pairplot\nimport seaborn as sns\ndef pairplot(df, target):\n    return sns.pairplot(df.sample(int(len(df/10000))),hue=target)\n    \n\n# create X,y variables for ML\nfrom sklearn.model_selection import train_test_split\ndef X_y_sets(df, target):\n    X = df.dropna().drop(columns=[target]).copy()\n    y = df.dropna()[target].ravel().copy()\n    \n    return train_test_split(X, y, test_size=0.33, random_state=42), X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n\n# classifier iteration\ndef classification_feat_importance(df_encoded):\n    \n    # iterate through each column variable as classification targets\n    for target in df_encoded.columns.values:\n        X = df_encoded.drop(columns=[target]).copy()\n        y = df_encoded[target].ravel().copy()\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n        \n    \n        # classifiers\n        #clf1 = GradientBoostingClassifier(criterion=\"friedman_mse\", init=None, learning_rate=0.3338, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n        #clf2 = GradientBoostingClassifier(criterion=\"squared_error\", init=None, learning_rate=0.2222, loss='deviance', max_depth=19, max_features=None, max_leaf_nodes=None, min_samples_leaf=6, min_samples_split=120, min_weight_fraction_leaf=0.0, n_estimators=500, random_state=42, subsample=1.0, verbose=1, warm_start=False).fit(X_train, y_train)\n        clf3 = RandomForestClassifier(max_depth=5, n_estimators=1000, random_state=42).fit(X_train, y_train)\n        clf4 = ExtraTreesClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n        clf5 = AdaBoostClassifier(n_estimators=8000, random_state=42).fit(X_train, y_train)\n        clf6 = MLPClassifier(alpha=1, max_iter=500).fit(X_train, y_train)\n        clf7 = KNeighborsClassifier(n_neighbors=9).fit(X_train, y_train)\n        classifiers = [\n                       #clf1, \n                       #clf2, \n                       clf3, \n                       clf4, \n                       clf5,\n                       clf6,\n                       #clf7\n                      ]\n\n        for classifier in classifiers:\n            results = []\n            results.append({\"classifier\":str(classifier).split(\"(\")[0],\"target\":target,\"test_score\":classifier.score(X_test, y_test)})\n            for i in results:\n                if target == 'verified':\n                    print(\"\\nClassifier:\",str(classifier).split(\"(\")[0],\"\\nTarget:\",target,\"\\nScore:\",classifier.score(X_test, y_test))\n        \n        test_matrix = confusion_matrix(y_test, clf.predict(X_test)) \n        results = pd.DataFrame(results)\n        \n    return results,test_matrix\n\nprint(\"To analyze which target-classifier would yield the best results: \\nUncomment (#) the code below.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encoding\nfrom sklearn.preprocessing import LabelEncoder\ndef encode(df):\n    lb_make = LabelEncoder()\n    columns = df.columns.values.tolist()\n    df_encoded = df[columns].copy()\n\n    # categorize/encode\n    for i in columns:\n        df_encoded[i] = lb_make.fit_transform(df[i])\n\n    # encoded\n    return df_encoded\n\n\n# encoded variable re-mapping\ndef encoding_remap(df, df_encoded, target):\n    \n    X_test = X_y_sets(df, target)[0][0]\n    \n    remap = pd.merge(df_encoded.loc[df_encoded.index.isin(X_test.index.values)][target].reset_index(),\n              df.loc[df.index.isin(X_test.index.values)][[target]].reset_index(),on=['index'])\n    \n    remap[target] = [str(remap[f'{target}_y'][i]) for i,v in remap[f'{target}_x'].items()]\n    remap['index'] = np.array([str(remap[f'{target}_x'][i]) for i,v in remap[f'{target}_x'].items()]).astype(int)\n    remap=remap[[target,'index']]\n    remap = remap.set_index('index').drop_duplicates().sort_values('index')\n    \n    return remap\n\n\n# pairplot\nimport seaborn as sns\ndef pairplot(df, target):\n    return sns.pairplot(df.sample(int(len(df/10000))),hue=target)\n    \n    \n# create X,y variables for ML\nfrom sklearn.model_selection import train_test_split\ndef X_y_sets(df, target):\n    X = df.dropna().drop(columns=[target]).copy()\n    y = df.dropna()[target].ravel().copy()\n    \n    return train_test_split(X, y, test_size=0.33, random_state=42), X, y\n\n\n# classifier iteration\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\ndef classification_feat_importance(df_encoded):\n    \n    df_encoded = df_encoded.drop(columns=['target1','target2'])\n    \n    # iterate through each column variable as classification targets\n    for target in df_encoded.columns.values:\n        X = df_encoded.dropna().drop(columns=[target]).copy()\n        y = df_encoded.dropna()[target].ravel().copy()\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n        \n        time.sleep(30)\n        \n        # classifiers\n        clf1 = RandomForestClassifier(max_depth=5, n_estimators=200, random_state=42).fit(X_train, y_train)\n        clf2 = AdaBoostClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n        clf3 = ExtraTreesClassifier(n_estimators=200, random_state=42).fit(X_train, y_train)\n        clf4 = KNeighborsClassifier().fit(X_train, y_train)\n        clf5 = MLPClassifier(alpha=1, max_iter=500).fit(X_train, y_train)\n        classifiers = [\n                       clf1, \n                       clf2, \n                       clf3, \n                       clf4, \n                       clf5\n                      ]\n\n        for classifier in classifiers:\n            results = []\n            #test_matrix = confusion_matrix(y_test, clf.predict(X_test))\n            results.append({\"classifier\":str(classifier).split(\"(\")[0],\"target\":target,\"test_score\":classifier.score(X_test, y_test)})\n            print(\"Classifier:\",str(classifier).split(\"(\")[0],\"\\t\\tTarget:\",target,\"\\tScore:\",classifier.score(X_test, y_test))\n            \n    return pd.DataFrame(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Variable Definitions & API Documentation\nData Columns in Kepler Objects of Interest Table <br>*https://exoplanetarchive.ipac.caltech.edu/docs/API_kepcandidate_columns.html\n\n\nExoplanet Archive Application Programming Interface (API) User Guide\n<br>*https://exoplanetarchive.ipac.caltech.edu/docs/program_interfaces.html","metadata":{}},{"cell_type":"markdown","source":"# Artificial Intelligence","metadata":{}},{"cell_type":"code","source":"# exoplanet_analysis.py\n\nimport requests\nimport pandas as pd\n\ndef fetch_nasa_exoplanet_data():\n    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+ps&format=csv\"\n    response = requests.get(url)\n    \n    with open(\"nasa_exoplanets.csv\", \"wb\") as file:\n        file.write(response.content)\n    \n    print(\"Data fetched and saved as nasa_exoplanets.csv\")\n\ndef main():\n    fetch_nasa_exoplanet_data()\n    # Add more functions to categorize exoplanets and leverage AI/ML/RL\n\n\ndef fetch_nasa_exoplanet_data():\n    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+ps&format=csv\"\n    response = requests.get(url)\n    \n    with open(\"nasa_exoplanets.csv\", \"wb\") as file:\n        file.write(response.content)\n    \n    print(\"Data fetched and saved as nasa_exoplanets.csv\")\n\ndef categorize_exoplanets():\n    df = pd.read_csv(\"nasa_exoplanets.csv\")\n    # Example criteria for categorization\n    habitable_zone = df[(df['pl_orbper'] > 200) & (df['pl_orbper'] < 400)]\n    habitable_zone.to_csv(\"habitable_exoplanets.csv\", index=False)\n    print(\"Categorized exoplanets and saved to habitable_exoplanets.csv\")\n\ndef main():\n    fetch_nasa_exoplanet_data()\n    categorize_exoplanets()\n    # Add more functions to leverage AI/ML/RL\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport joblib\n\ndef fetch_nasa_exoplanet_data():\n    url = \"https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+ps&format=csv\"\n    response = requests.get(url)\n    \n    with open(\"nasa_exoplanets.csv\", \"wb\") as file:\n        file.write(response.content)\n    \n    print(\"Data fetched and saved as nasa_exoplanets.csv\")\n\ndef categorize_exoplanets():\n    df = pd.read_csv(\"nasa_exoplanets.csv\")\n    habitable_zone = df[(df['pl_orbper'] > 200) & (df['pl_orbper'] < 400)]\n    habitable_zone.to_csv(\"habitable_exoplanets.csv\", index=False)\n    print(\"Categorized exoplanets and saved to habitable_exoplanets.csv\")\n\ndef train_ml_model():\n    df = pd.read_csv(\"habitable_exoplanets.csv\")\n    features = df[['pl_orbper', 'pl_rade', 'pl_bmasse']]  # Example features\n    labels = df['habitable']  # This column needs to be defined based on your criteria\n\n    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    \n    joblib.dump(model, \"exoplanet_model.pkl\")\n    print(\"Model trained and saved as exoplanet_model.pkl\")\n\ndef main():\n    fetch_nasa_exoplanet_data()\n    categorize_exoplanets()\n    train_ml_model()","metadata":{"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas scikit-learn requests joblib","metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python exoplanet_analysis.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#en fin","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}